{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae7e2a60-fa48-41f2-b7cb-1c9ac25d56a9",
   "metadata": {},
   "source": [
    "# 4 - Data Preprocessing (Part 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06972390-82ac-414b-a08d-48ebdca1083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import rt # remove tweets\n",
    "\n",
    "df = pd.read_csv(\"inputs/tweets_2.csv\") # import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adaa1fd-b468-423d-82c9-81515cca67cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75f6013-a53b-488c-8106-34cc26c6f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b85dc3-77b6-4448-9eb3-5bae37136d5a",
   "metadata": {},
   "source": [
    "## Some tweets that were not translated are missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c90c69b-515f-411c-9318-839fd5fc1102",
   "metadata": {},
   "source": [
    "* Some tweets that were not translated are missing values.\n",
    "* Can't convert missing values to lowercase letters.\n",
    "* There are 6 missing values.\n",
    "* Replace/Substitute missing vaues with `content_4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca90905-8bdd-4704-92d0-ab53f1724ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df[df[\"content_5\"].isna()]\n",
    "missing_values.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7babf592-021b-44e2-8a41-9a5565886a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[missing_values.index, \"content_5\"] = df.loc[missing_values.index, \"content_4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2439c4-b328-4ea8-b5aa-15607a4a1d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df[df[\"content_5\"].isna()]\n",
    "missing_values.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a55abd1-4999-4ee9-8394-a1c8635c1758",
   "metadata": {},
   "source": [
    "## 2.1. Case folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e66bbc-6d65-414d-996d-84bddf59a26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"content_5\"] = df[\"content_5\"].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45486865-420e-4cae-9af5-e219af6e3279",
   "metadata": {},
   "source": [
    "## 2.2. Remove hastags, mentions, links, and non-alphabetical characters (including numerical characters and special characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e5c675-53c4-41a1-9161-a53337c242a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "def remove(x):\n",
    "    \n",
    "    # remove non-white space characters after # (hashtags)\n",
    "    # remove non-white space characters after @ (mentions)\n",
    "    # remove non-white space characters after http (links)\n",
    "    x = re.sub(\"(\\#\\S+)|(\\@\\S+)|(http\\S+)\", \"\", x)\n",
    "    \n",
    "    # substitute non-alphabetical characters into a single space\n",
    "    x = re.sub(\"([^a-z])\", \" \", x)\n",
    "    \n",
    "    # substitute multiple spaces into a single space\n",
    "    x = re.sub(\"(\\s+)\", \" \", x)\n",
    "    \n",
    "    x = x.strip()\n",
    "    \n",
    "    return x\n",
    "\n",
    "df[\"content_6\"] = df[\"content_5\"].apply(lambda x: remove(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c101d3d-67ce-4845-a799-4b2e5e79fb76",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.3. Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f835cbd2-bf22-4a37-974f-352466dac204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# english stop words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw_english = stopwords.words(\"english\")\n",
    "\n",
    "# tagalog stop words\n",
    "\n",
    "import advertools\n",
    "\n",
    "sw_tagalog = list(advertools.stopwords[\"tagalog\"])\n",
    "\n",
    "# domain stop words\n",
    "\n",
    "sw_domain = [\"converge\", \"globe\", \"pldt\"]\n",
    "\n",
    "# stop words\n",
    "\n",
    "sw = sw_english + sw_tagalog + sw_domain\n",
    "\n",
    "sw = [remove(i) for i in sw]\n",
    "\n",
    "# remove stop words\n",
    "\n",
    "df[\"content_7\"] = df[\"content_6\"].apply(lambda x: \" \".join([i for i in x.split() if i not in sw]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9df5e67-3d48-4bac-aab5-34c082481595",
   "metadata": {},
   "source": [
    "## 2.4 Remove low quality words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26122152-97b1-40d8-8548-cbc15a74fb4f",
   "metadata": {},
   "source": [
    "* Remove words with a character count less than or equal to 2 and greater than or equal to 30.\n",
    "* Remove words that are permutations of one or two alphabetical characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe1a3b6-7f94-4d62-9618-6d4f8b85f768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_low_quality_words(x):\n",
    "    \n",
    "    lst = list()\n",
    "    \n",
    "    for i in x.split():\n",
    "        if len(i) > 2:\n",
    "            if len(i) < 30:\n",
    "                if len(set(i)) > 2:\n",
    "                    lst.append(i)\n",
    "                    \n",
    "    return \" \".join(lst)\n",
    "\n",
    "df[\"content_8\"] = df[\"content_7\"].apply(lambda x: remove_low_quality_words(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee686867-3462-4314-811a-b5a5a7f86dad",
   "metadata": {},
   "source": [
    "## Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec03c9d6-391a-4002-b117-586f996550be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "terms = \" \".join(df[\"content_8\"])\n",
    "\n",
    "term_frequency = dict()\n",
    "\n",
    "for term in terms.split():\n",
    "    if term not in term_frequency:\n",
    "        term_frequency[term] = 1\n",
    "    else:\n",
    "        term_frequency[term] = term_frequency[term] + 1\n",
    "        \n",
    "tf = pd.DataFrame.from_dict(term_frequency, orient = \"index\")\n",
    "\n",
    "tf.reset_index(inplace = True)\n",
    "\n",
    "tf.rename(columns = {\"index\" : \"term\", 0 : \"frequency\"}, inplace = True)\n",
    "\n",
    "tf.sort_values(by = [\"frequency\", \"term\"], ascending = [0, 1], inplace = True, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6261abe-1417-4adb-990a-8c55568b6d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf[tf.frequency >= tf.frequency.quantile(q = 0.9999)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e680c83-4214-4509-9fd9-75ed55f50b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf[tf.frequency <= tf.frequency.quantile(q = 0.0100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8fb879-acb3-4f83-a788-fe3e6a7d4964",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(len(tf[tf[\"frequency\"] == 1]) / len(tf) * 100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede51aa8-4989-4cb3-8c5a-e7fe5a228b8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.5. Remove low frequency words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05465a58-e3b5-426a-a134-0ab134dc37ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_frequency_words = list(tf[tf[\"frequency\"] == 1][\"term\"])\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "lst = list()\n",
    "\n",
    "for i in tqdm(range(len(df))):\n",
    "    lst.append(\" \".join([i for i in df[\"content_8\"][i].split() if i not in low_frequency_words]))\n",
    "    \n",
    "df[\"content_9\"] = lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0037f27-a64e-4d54-9e39-901d40d24598",
   "metadata": {},
   "source": [
    "## 2.6. Filter by word count 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7ce909-c4ac-4f67-a49d-e9ccce380a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"word_count_2\"] = df[\"content_9\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "df = rt.remove_tweets(df = df,\n",
    "                      condition = df[\"word_count_2\"] > 2,\n",
    "                      column = \"content_9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c959ff9-35b3-40b9-a991-c213bd437ebc",
   "metadata": {},
   "source": [
    "## 2.7. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e4748b0a-159f-4ff7-8c5e-4066dadb8446",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatization(x):\n",
    "    \n",
    "    words = list()\n",
    "    \n",
    "    for i in x.split():\n",
    "        word = i\n",
    "        \n",
    "        for j in [\"a\", \"n\", \"r\", \"s\", \"v\"]:\n",
    "            word = lemmatizer.lemmatize(word, pos = j)\n",
    "            \n",
    "        words.append(word)\n",
    "        \n",
    "    return \" \".join(words)\n",
    "\n",
    "df[\"content_10\"] = df[\"content_9\"].apply(lambda x: lemmatization(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de39d095-d176-4249-a73e-599b755df50f",
   "metadata": {},
   "source": [
    "## `month`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d47757-e398-4060-b407-f8cf0d12cbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"month\"] = df[\"date\"].apply(lambda x: int(x[5:7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492bdd63-909c-4b83-82f6-9e2731405f13",
   "metadata": {},
   "source": [
    "## `company_x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f52170-c9da-42bd-bd02-3acb16bbb6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df[\"company_x\"] = np.select(condlist = [df[\"company_a\"] == 1, df[\"company_b\"] == 1, df[\"company_c\"] == 1],\n",
    "                            choicelist = [\"a\", \"b\", \"c\"],\n",
    "                            default = np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9b1a8d-e0e3-4579-9e72-cd187a4ac425",
   "metadata": {},
   "outputs": [],
   "source": [
    "rt.rt(345667, 236956)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3a08ab-1a7b-4802-93cc-62866ab4bea4",
   "metadata": {},
   "source": [
    "## Export data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df14d7a2-dd6e-4fba-b821-5a4f73690a74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df.to_csv(\"inputs/tweets_3.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d28470e-1eed-4031-ad5e-698e6b3674c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc6a31f-ff2b-43ec-8620-985432289c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
